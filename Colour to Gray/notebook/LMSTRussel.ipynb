{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "i17wcxg4oHqz"
   },
   "source": [
    "Name: Saman Akhtar USC ID: 9944619932"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WSWnOhQ-oDoO"
   },
   "source": [
    "**Imports**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "Mv4iLzU4c90D"
   },
   "outputs": [],
   "source": [
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, LSTM\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "from keras.utils import np_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_seNng6ZpWVu",
    "outputId": "08c45b93-1afa-48ac-9f42-9be094134a19"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num GPUs Available:  1\n"
     ]
    }
   ],
   "source": [
    "print(\"Num GPUs Available: \", \n",
    "      len(tf.config.experimental.list_physical_devices('GPU')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "KhFtdm5Kc90F",
    "outputId": "df7aa164-d64a-4594-d0a4-8ca9003ca870"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "#drive.mount('../data/books')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "rJxyjrJ9ql1x"
   },
   "outputs": [],
   "source": [
    "drive_folder = '/content/drive/My Drive/ML/homework-7-SamanUSC'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3FxOS71S_G8k"
   },
   "source": [
    "# **Generative Models for Text**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ymCL4CFi-0U4"
   },
   "source": [
    " ## **Download the following books from Project Gutenberg http://www.gutenberg.\n",
    "## org/ebooks/author/355 in text format:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kwBFqYWDD9ci"
   },
   "source": [
    "###  LSTM: Train an LSTM to mimic Russell’s style and thoughts:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "08kRIq6-EPEi"
   },
   "source": [
    "#### i. Concatenate your text files to create a corpus of Russell’s writings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "1TP1PNOaD89H"
   },
   "outputs": [],
   "source": [
    "def read_books(dir):\n",
    "  print(\" The current directory\", dir)\n",
    "\n",
    "  # Change the directory\n",
    "  os.chdir(dir)\n",
    "  corpus_lst = []\n",
    "\n",
    "  # Iterate through all file\n",
    "  for file in os.listdir():\n",
    "\n",
    "      # Check whether file is in the text format or not\n",
    "      if file.endswith(\".txt\"):\n",
    "        file_path = f\"{dir}/{file}\"\n",
    "   \n",
    "      with open(file_path, encoding = 'ascii', errors = 'ignore') as corp:\n",
    "\n",
    "        # Reading all the text in lower case\n",
    "        corpus = corp.read().lower()\n",
    "        corpus_lst.append(corpus)\n",
    "      print(\" Read the text of the book:\", file, \" of the length: \", len(corpus_lst[-1]))\n",
    "  corpus_lst = sorted(corpus_lst, key=lambda x : len(x))\n",
    "  return corpus_lst \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1ganRyP3-ze9",
    "outputId": "630c7363-6e73-488b-db17-60538c816d17"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " The current directory /content/drive/My Drive/ML/homework-7-SamanUSC/data/books/\n",
      " Read the text of the book: TAMatter.txt  of the length:  766542\n",
      " Read the text of the book: THWP.txt  of the length:  2005566\n",
      " Read the text of the book: TPP.txt  of the length:  244306\n",
      " Read the text of the book: MLOE.txt  of the length:  412226\n",
      " Read the text of the book: OKEWFSMP.txt  of the length:  405741\n",
      " Read the text of the book: TAM.txt  of the length:  514652\n",
      " Read the text of the book: AIIMAT.txt  of the length:  746219\n"
     ]
    }
   ],
   "source": [
    "# Create corpus of Russell's writings\n",
    "all_corpus = read_books(drive_folder + '/data/books/')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OB55hC9WEe82"
   },
   "source": [
    "#### ii. Use a character-level representation for this model by using extended ASCII that has N = 256 characters. Each character will be encoded into a an integer using its ASCII code. Rescale the integers to the range [0, 1], because LSTM uses a sigmoid activation function. LSTM will receive the rescaled integers as its input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "Av_XOQs3ernr"
   },
   "outputs": [],
   "source": [
    "def getRepresentation(all_corpus):\n",
    "  all_char = []\n",
    "  for corpus in all_corpus:\n",
    "\n",
    "      # Get 1 set of all characters in the corpus   \n",
    "      all_char.extend(list(set(set(corpus))))\n",
    "  all_char = sorted(list(set(all_char)))\n",
    "  # rem_char= ['|' ,'#', '%' , '~', '$', '+','^']\n",
    "  # all_char = set(all_char) - set(rem_char)\n",
    "  charInt = dict((c, i) for i, c in enumerate(all_char))\n",
    "  intChar = dict((i, c) for i, c in enumerate(all_char))\n",
    "  return charInt, intChar, all_char"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "DKFET2iWkEIp",
    "outputId": "baf47270-1e18-4150-ad0a-d377cc39d951"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'\\n': 0, ' ': 1, '!': 2, '\"': 3, '#': 4, '$': 5, '%': 6, '&': 7, \"'\": 8, '(': 9, ')': 10, '*': 11, '+': 12, ',': 13, '-': 14, '.': 15, '/': 16, '0': 17, '1': 18, '2': 19, '3': 20, '4': 21, '5': 22, '6': 23, '7': 24, '8': 25, '9': 26, ':': 27, ';': 28, '<': 29, '=': 30, '>': 31, '?': 32, '[': 33, '\\\\': 34, ']': 35, '^': 36, '_': 37, 'a': 38, 'b': 39, 'c': 40, 'd': 41, 'e': 42, 'f': 43, 'g': 44, 'h': 45, 'i': 46, 'j': 47, 'k': 48, 'l': 49, 'm': 50, 'n': 51, 'o': 52, 'p': 53, 'q': 54, 'r': 55, 's': 56, 't': 57, 'u': 58, 'v': 59, 'w': 60, 'x': 61, 'y': 62, 'z': 63, '{': 64, '|': 65, '}': 66, '~': 67}\n",
      "{0: '\\n', 1: ' ', 2: '!', 3: '\"', 4: '#', 5: '$', 6: '%', 7: '&', 8: \"'\", 9: '(', 10: ')', 11: '*', 12: '+', 13: ',', 14: '-', 15: '.', 16: '/', 17: '0', 18: '1', 19: '2', 20: '3', 21: '4', 22: '5', 23: '6', 24: '7', 25: '8', 26: '9', 27: ':', 28: ';', 29: '<', 30: '=', 31: '>', 32: '?', 33: '[', 34: '\\\\', 35: ']', 36: '^', 37: '_', 38: 'a', 39: 'b', 40: 'c', 41: 'd', 42: 'e', 43: 'f', 44: 'g', 45: 'h', 46: 'i', 47: 'j', 48: 'k', 49: 'l', 50: 'm', 51: 'n', 52: 'o', 53: 'p', 54: 'q', 55: 'r', 56: 's', 57: 't', 58: 'u', 59: 'v', 60: 'w', 61: 'x', 62: 'y', 63: 'z', 64: '{', 65: '|', 66: '}', 67: '~'}\n"
     ]
    }
   ],
   "source": [
    "charInt, intChar, all_char = getRepresentation(all_corpus)\n",
    "print(charInt)\n",
    "print(intChar)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nbVKFQ-XzE2Q"
   },
   "source": [
    "#### iii) Choose a window size, iv. Inputs to the network will be the first W −1 = 99 characters of each sequence, and the output of the network will be the Wth character of the sequence. Basically, we are training the network to predict each character using the 99 characters that precede it. Slide the window in strides of S = 1 on the text.For example, if W = 5 and S = 1 and we want to train the network with the sequence ABRACADABRA, The first input to the network will be ABRA and the corresponding output will be C. The second input will be BRAC and the second output will be A, etc.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "_kl8Z9KI4unk"
   },
   "outputs": [],
   "source": [
    "def windCorpus(corpus, charDict, wind_size ):\n",
    "    input = []\n",
    "    output = []\n",
    "    for w in range(0, len(corpus) - wind_size + 1, 1):\n",
    "\n",
    "        # Storing  W − 1 characters of each sequence as input\n",
    "        seqIn = corpus[w : w + wind_size - 1]\n",
    "\n",
    "        # storing W character of each sequence as output\n",
    "        seqOut = corpus[w + wind_size - 1]\n",
    "        input.append([charDict[c] for c in seqIn])\n",
    "        output.append(charDict[seqOut])\n",
    "    print(len(output))\n",
    "    return input, output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "4pQqEUHs3u9O"
   },
   "outputs": [],
   "source": [
    "def genInOut(corpus, charDict, wind_size = 100, n = 7):\n",
    "  inSeq = []\n",
    "  outChar = []\n",
    "  for book in corpus[:n]:\n",
    "    input, output = windCorpus(book, charDict, wind_size)\n",
    "    inSeq.extend(input)\n",
    "    outChar.extend(output)\n",
    "  return inSeq, outChar"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zPr6PEiq3OCf"
   },
   "source": [
    "Choosing window size as 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "geCxUzhX4d7E",
    "outputId": "2ef327f6-04b7-48a1-e365-b37e4d3311d4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "244207\n",
      "405642\n",
      "412127\n",
      "514553\n",
      "746120\n"
     ]
    }
   ],
   "source": [
    "wind_size = 100\n",
    "n = 5\n",
    "inSeq, outChar = genInOut(all_corpus, charInt, wind_size, n)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OD5Q1-BE1k-U"
   },
   "source": [
    "#### v. Note that the output has to be encoded using a one-hot encoding scheme with N = 256 (or less) elements. This means that the network reads integers, but outputs a vector of N = 256 (or less) elements. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "g9xzjvN43M7O",
    "outputId": "17113367-35d1-4953-e4f2-44904c81cbb0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2322649\n",
      "(2322649, 99, 1)\n"
     ]
    }
   ],
   "source": [
    "print(len(inSeq))\n",
    "# Reshape\n",
    "# input to LSTM layer should be in 3D shape (samples, time-steps, features)\n",
    "lstm_in = np.reshape(inSeq, (len(inSeq), wind_size - 1, 1))\n",
    "print(lstm_in.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "5rldks8O0E9Y"
   },
   "outputs": [],
   "source": [
    "# Normalize\n",
    "lstm_in = lstm_in / float(len(charInt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "WJ5c8TserNSj",
    "outputId": "4e6e186b-c0dd-41b7-c323-2ada7a7ea7fa"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2322649, 68)\n"
     ]
    }
   ],
   "source": [
    "lstm_out = np_utils.to_categorical(outChar)\n",
    "print(lstm_out.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "O6-_etSQ1ktZ"
   },
   "source": [
    "#### vi. Use a single hidden layer for the LSTM withN= 256 (or less) memory units.vii. Use a Softmax output layer to yield a probability prediction for each of thecharacters between 0 and . This is actually a character classification problemwithNclasses. Choose log loss (cross entropy) as the objective function forthe network ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "eJ4MaLTD0w1B",
    "outputId": "0721afec-a37b-4948-99a0-f068ea78d880"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm (LSTM)                  (None, 256)               264192    \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 68)                17476     \n",
      "=================================================================\n",
      "Total params: 281,668\n",
      "Trainable params: 281,668\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# Build LSTM Sequential model\n",
    "model = Sequential()\n",
    "\n",
    "# Input to LSTM layer should be in 3D shape (samples, time-steps, features)\n",
    "model.add(LSTM(256, input_shape=(lstm_in.shape[1], lstm_in.shape[2])))\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "# Add one LSTM layer.\n",
    "model.add(Dense(lstm_out.shape[1], activation='softmax'))\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jB9T3N8IBUCd"
   },
   "source": [
    "#### viii. We do not use a test dataset. We are using the whole training dataset tolearn the probability of each character in a sequence. We are not seeking fora very accurate model. Instead we are interested in a generalization of thedataset that can mimic the gist of the text.ix. Choose a reasonable number of epochs4for training, considering your compu-tational power .x. Use model checkpointing to keep the network weights to determine each timean improvement in loss is observed at the end of the epoch. Find the best setof weights in terms of loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "ON-6pqTxrNVy"
   },
   "outputs": [],
   "source": [
    "# Configuring models learning process with .compile():\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam')\n",
    "filepath = drive_folder + \"/LSTMweights/weights-improvement-{epoch:02d}-{loss:.2f}-bigger.hdf5\"\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='loss', verbose=1, save_best_only=True, mode='min')\n",
    "callbacks_list = [checkpoint]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4zD5w9dEBfas"
   },
   "source": [
    "Choosing epoch as 25 and batch size as 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "JSBMjJMorNZY",
    "outputId": "051a311a-8a2e-4f7e-cf98-87f5eaf0cdb4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "18146/18146 [==============================] - 264s 14ms/step - loss: 2.6697\n",
      "\n",
      "Epoch 00001: loss improved from inf to 2.66970, saving model to /content/drive/My Drive/ML/homework-7-SamanUSC/LSTMweights/weights-improvement-01-2.67-bigger.hdf5\n",
      "Epoch 2/20\n",
      "18146/18146 [==============================] - 258s 14ms/step - loss: 2.3157\n",
      "\n",
      "Epoch 00002: loss improved from 2.66970 to 2.31574, saving model to /content/drive/My Drive/ML/homework-7-SamanUSC/LSTMweights/weights-improvement-02-2.32-bigger.hdf5\n",
      "Epoch 3/20\n",
      "18146/18146 [==============================] - 258s 14ms/step - loss: 2.1182\n",
      "\n",
      "Epoch 00003: loss improved from 2.31574 to 2.11822, saving model to /content/drive/My Drive/ML/homework-7-SamanUSC/LSTMweights/weights-improvement-03-2.12-bigger.hdf5\n",
      "Epoch 4/20\n",
      "18146/18146 [==============================] - 258s 14ms/step - loss: 2.1737\n",
      "\n",
      "Epoch 00004: loss did not improve from 2.11822\n",
      "Epoch 5/20\n",
      "18146/18146 [==============================] - 258s 14ms/step - loss: 2.9647\n",
      "\n",
      "Epoch 00005: loss did not improve from 2.11822\n",
      "Epoch 6/20\n",
      "18146/18146 [==============================] - 257s 14ms/step - loss: 2.6113\n",
      "\n",
      "Epoch 00006: loss did not improve from 2.11822\n",
      "Epoch 7/20\n",
      "18146/18146 [==============================] - 257s 14ms/step - loss: 2.3763\n",
      "\n",
      "Epoch 00007: loss did not improve from 2.11822\n",
      "Epoch 8/20\n",
      "18146/18146 [==============================] - 257s 14ms/step - loss: 2.2646\n",
      "\n",
      "Epoch 00008: loss did not improve from 2.11822\n",
      "Epoch 9/20\n",
      "18146/18146 [==============================] - 258s 14ms/step - loss: 2.7597\n",
      "\n",
      "Epoch 00009: loss did not improve from 2.11822\n",
      "Epoch 10/20\n",
      "18146/18146 [==============================] - 257s 14ms/step - loss: 2.5159\n",
      "\n",
      "Epoch 00010: loss did not improve from 2.11822\n",
      "Epoch 11/20\n",
      "18146/18146 [==============================] - 257s 14ms/step - loss: 2.2263\n",
      "\n",
      "Epoch 00011: loss did not improve from 2.11822\n",
      "Epoch 12/20\n",
      "18146/18146 [==============================] - 258s 14ms/step - loss: 2.0747\n",
      "\n",
      "Epoch 00012: loss improved from 2.11822 to 2.07473, saving model to /content/drive/My Drive/ML/homework-7-SamanUSC/LSTMweights/weights-improvement-12-2.07-bigger.hdf5\n",
      "Epoch 13/20\n",
      "18146/18146 [==============================] - 257s 14ms/step - loss: 1.9863\n",
      "\n",
      "Epoch 00013: loss improved from 2.07473 to 1.98631, saving model to /content/drive/My Drive/ML/homework-7-SamanUSC/LSTMweights/weights-improvement-13-1.99-bigger.hdf5\n",
      "Epoch 14/20\n",
      "18146/18146 [==============================] - 257s 14ms/step - loss: 1.9314\n",
      "\n",
      "Epoch 00014: loss improved from 1.98631 to 1.93139, saving model to /content/drive/My Drive/ML/homework-7-SamanUSC/LSTMweights/weights-improvement-14-1.93-bigger.hdf5\n",
      "Epoch 15/20\n",
      "18146/18146 [==============================] - 257s 14ms/step - loss: 1.8914\n",
      "\n",
      "Epoch 00015: loss improved from 1.93139 to 1.89139, saving model to /content/drive/My Drive/ML/homework-7-SamanUSC/LSTMweights/weights-improvement-15-1.89-bigger.hdf5\n",
      "Epoch 16/20\n",
      "18146/18146 [==============================] - 258s 14ms/step - loss: 1.8606\n",
      "\n",
      "Epoch 00016: loss improved from 1.89139 to 1.86058, saving model to /content/drive/My Drive/ML/homework-7-SamanUSC/LSTMweights/weights-improvement-16-1.86-bigger.hdf5\n",
      "Epoch 17/20\n",
      "18146/18146 [==============================] - 258s 14ms/step - loss: 1.8347\n",
      "\n",
      "Epoch 00017: loss improved from 1.86058 to 1.83475, saving model to /content/drive/My Drive/ML/homework-7-SamanUSC/LSTMweights/weights-improvement-17-1.83-bigger.hdf5\n",
      "Epoch 18/20\n",
      "18146/18146 [==============================] - 258s 14ms/step - loss: 1.8133\n",
      "\n",
      "Epoch 00018: loss improved from 1.83475 to 1.81333, saving model to /content/drive/My Drive/ML/homework-7-SamanUSC/LSTMweights/weights-improvement-18-1.81-bigger.hdf5\n",
      "Epoch 19/20\n",
      "18146/18146 [==============================] - 257s 14ms/step - loss: 1.7950\n",
      "\n",
      "Epoch 00019: loss improved from 1.81333 to 1.79501, saving model to /content/drive/My Drive/ML/homework-7-SamanUSC/LSTMweights/weights-improvement-19-1.80-bigger.hdf5\n",
      "Epoch 20/20\n",
      "18146/18146 [==============================] - 258s 14ms/step - loss: 1.7787\n",
      "\n",
      "Epoch 00020: loss improved from 1.79501 to 1.77872, saving model to /content/drive/My Drive/ML/homework-7-SamanUSC/LSTMweights/weights-improvement-20-1.78-bigger.hdf5\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f2d7054ef90>"
      ]
     },
     "execution_count": 23,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# fit the model\n",
    "model.fit(lstm_in, lstm_out, epochs = 20, batch_size = 128, callbacks = callbacks_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BB3_ZtKsD4FV"
   },
   "source": [
    "#### xi Use the network with the best weights to generate 1000 characters, using thefollowing text as initialization of the network:There are those who take mental phenomena naively, just as theywould physical phenomena. This school of psychologists tends not toemphasize the object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "id": "UrCqIuLq1jMv"
   },
   "outputs": [],
   "source": [
    "txt = 'There are those who take mental phenomena naively, just as they would physical phenomena. This school of psychologists tends not to emphasize the object.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "id": "JgJs2W0AG5gP"
   },
   "outputs": [],
   "source": [
    "text_int = [charInt[c] for c in txt[-99:].lower()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "dwabBWrWq9xV"
   },
   "outputs": [],
   "source": [
    "\n",
    "def generateNewTxt(txt , text_int, charInt, intChar, model, ch_len = 1000):\n",
    "  for i in range(ch_len):\n",
    "\n",
    "    # Reshape to 3D array\n",
    "    seq = np.reshape(text_int, (1, len(text_int), 1))\n",
    "\n",
    "    # Nomalize it\n",
    "    seq = seq / float(len(charInt))\n",
    "\n",
    "    # Predict the next character\n",
    "    nxt_char = model.predict(seq, verbose=0)\n",
    "    char_index = np.argmax(nxt_char)\n",
    "    txt += intChar[char_index]\n",
    "\n",
    "    # Generate new input sequence\n",
    "    text_int.append(char_index)\n",
    "    text_int = text_int[1:len(text_int)]\n",
    "  return txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "oJCHddpgH1u5"
   },
   "outputs": [],
   "source": [
    "new_txt = generateNewTxt(txt , text_int, charInt, intChar, model,  1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "AEeL0zdyIOxZ",
    "outputId": "4432f02f-9cb0-4b56-95db-f377f4963917"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " The new text generated by LSTM is \n",
      ": There are those who take mental phenomena naively, just as they would physical phenomena. This school of psychologists tends not to emphasize the object. the semsetier of the sense of the sense of the sense of the sense of the sense of the sense of the sense of the sense of the sense of the sense of the sense of the sense of the sense of the sense of the sense of the sense of the sense of the sense of the sense of the sense of the sense of the sense of the sense of the sense of the sense of the sense of the sense of the sense of the sense of the sense of the sense of the sense of the sense of the sense of the sense of the sense of the sense of the sense of the sense of the sense of the sense of the sense of the sense of the sense of the sense of the sense of the sense of the sense of the sense of the sense of the sense of the sense of the sense of the sense of the sense of the sense of the sense of the sense of the sense of the sense of the sense of the sense of the sense of the sense of the sense of the sense of the sense of the sense of the sense of the sense of the sense of the sense of the sense of the sense of the sense of the sen\n"
     ]
    }
   ],
   "source": [
    "print(\" The new text generated by LSTM is \\n:\", new_txt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xTwijjBFJb-w"
   },
   "source": [
    "Fitting model with more epoc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "g9D1WT8frVFN",
    "outputId": "b0c37c9f-4210-43f1-933f-35848de4d1a2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "18146/18146 [==============================] - 258s 14ms/step - loss: 1.7655\n",
      "\n",
      "Epoch 00001: loss improved from 1.77872 to 1.76555, saving model to /content/drive/My Drive/ML/homework-7-SamanUSC/LSTMweights/weights-improvement-01-1.77-bigger.hdf5\n",
      "Epoch 2/10\n",
      "18146/18146 [==============================] - 257s 14ms/step - loss: 1.7530\n",
      "\n",
      "Epoch 00002: loss improved from 1.76555 to 1.75303, saving model to /content/drive/My Drive/ML/homework-7-SamanUSC/LSTMweights/weights-improvement-02-1.75-bigger.hdf5\n",
      "Epoch 3/10\n",
      "18146/18146 [==============================] - 257s 14ms/step - loss: 1.7424\n",
      "\n",
      "Epoch 00003: loss improved from 1.75303 to 1.74238, saving model to /content/drive/My Drive/ML/homework-7-SamanUSC/LSTMweights/weights-improvement-03-1.74-bigger.hdf5\n",
      "Epoch 4/10\n",
      "18146/18146 [==============================] - 257s 14ms/step - loss: 1.7322\n",
      "\n",
      "Epoch 00004: loss improved from 1.74238 to 1.73216, saving model to /content/drive/My Drive/ML/homework-7-SamanUSC/LSTMweights/weights-improvement-04-1.73-bigger.hdf5\n",
      "Epoch 5/10\n",
      "18146/18146 [==============================] - 258s 14ms/step - loss: 1.7237\n",
      "\n",
      "Epoch 00005: loss improved from 1.73216 to 1.72370, saving model to /content/drive/My Drive/ML/homework-7-SamanUSC/LSTMweights/weights-improvement-05-1.72-bigger.hdf5\n",
      "Epoch 6/10\n",
      "18146/18146 [==============================] - 259s 14ms/step - loss: 1.7143\n",
      "\n",
      "Epoch 00006: loss improved from 1.72370 to 1.71428, saving model to /content/drive/My Drive/ML/homework-7-SamanUSC/LSTMweights/weights-improvement-06-1.71-bigger.hdf5\n",
      "Epoch 7/10\n",
      "18146/18146 [==============================] - 260s 14ms/step - loss: 1.7067\n",
      "\n",
      "Epoch 00007: loss improved from 1.71428 to 1.70667, saving model to /content/drive/My Drive/ML/homework-7-SamanUSC/LSTMweights/weights-improvement-07-1.71-bigger.hdf5\n",
      "Epoch 8/10\n",
      "18146/18146 [==============================] - 258s 14ms/step - loss: 1.7032\n",
      "\n",
      "Epoch 00008: loss improved from 1.70667 to 1.70319, saving model to /content/drive/My Drive/ML/homework-7-SamanUSC/LSTMweights/weights-improvement-08-1.70-bigger.hdf5\n",
      "Epoch 9/10\n",
      "18146/18146 [==============================] - 260s 14ms/step - loss: 1.6986\n",
      "\n",
      "Epoch 00009: loss improved from 1.70319 to 1.69861, saving model to /content/drive/My Drive/ML/homework-7-SamanUSC/LSTMweights/weights-improvement-09-1.70-bigger.hdf5\n",
      "Epoch 10/10\n",
      "18146/18146 [==============================] - 262s 14ms/step - loss: 1.6924\n",
      "\n",
      "Epoch 00010: loss improved from 1.69861 to 1.69244, saving model to /content/drive/My Drive/ML/homework-7-SamanUSC/LSTMweights/weights-improvement-10-1.69-bigger.hdf5\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f2d6a19bfd0>"
      ]
     },
     "execution_count": 26,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(lstm_in, lstm_out, epochs = 10, batch_size = 128, callbacks = callbacks_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "id": "sBXH3ai_KNZX"
   },
   "outputs": [],
   "source": [
    "newtxt = generateNewTxt(txt , text_int, charInt, intChar, model,  1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "PXLcWOHlKNZd",
    "outputId": "30687059-7af7-42e4-c9e5-621e4efd9bb7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " The new text generated by LSTM is \n",
      ": There are those who take mental phenomena naively, just as they would physical phenomena. This school of psychologists tends not to emphasize the object.the senset and the senset and the senset and the senset and the senset and the senset and the senset and the senset and the senset and the senset and the senset and the senset and the senset and the senset and the senset and the senset and the senset and the senset and the senset and the senset and the senset and the senset and the senset and the senset and the senset and the senset and the senset and the senset and the senset and the senset and the senset and the senset and the senset and the senset and the senset and the senset and the senset and the senset and the senset and the senset and the senset and the senset and the senset and the senset and the senset and the senset and the senset and the senset and the senset and the senset and the senset and the senset and the senset and the senset and the senset and the senset and the senset and the senset and the senset and the senset and the senset and the senset and the senset and the senset and the senset and the senset and the senset\n"
     ]
    }
   ],
   "source": [
    "print(\" The new text generated by LSTM is \\n:\", newtxt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qzND7DkWFiE4"
   },
   "source": [
    "**Referrences**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QCBUK4hQFqsh"
   },
   "source": [
    "https://www.geeksforgeeks.org/how-to-read-multiple-text-files-from-folder-in-python/ https://keras.io/guides/functional_api/ https://www.tensorflow.org/tutorials/keras/ https://stackabuse.com/solving-sequence-problems-with-lstm-in-keras https://machinelearningmastery.com/reshape-input-data-long-short-term-memory-networks-keras/\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "LMST.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
